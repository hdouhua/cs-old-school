```
error: unable to upgrade connection: pod does not exist
```


```shell
k get node -o wide
#
NAME   STATUS   ROLES                  AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
vm1    Ready    control-plane,master   3d8h   v1.23.9   10.0.3.15     <none>        Ubuntu 22.04.1 LTS   5.15.0-46-generic   docker://20.10.17
vm2    Ready    <none>                 3d6h   v1.23.9   10.0.3.15     <none>        Ubuntu 22.04.1 LTS   5.15.0-46-generic   docker://20.10.17
```

发现 INTERNAL-IP 相同

vim /etc/default/kubelet
KUBELET_EXTRA_ARGS="--node-ip=192.168.56.101"
KUBELET_EXTRA_ARGS="--node-ip=192.168.56.103"
#or 
vim /etc/systemd/system/kubelet.service
--node-ip=192.168.56.101
--node-ip=192.168.56.103

sudo systemctl daemon-reload
sudo systemctl restart kubelet



k get pod -n kube-system
k get pod --all-namespaces

```shell
kubeadm reset
#
#---
[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
#---

sudo rm -rf /etc/cni/net.d

#to reset iptables, you must do so manually:
sudo iptables -L
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X

sudo rm -rf $HOME/.kube/

sudo dhclient -r enp0s8 -v
sudo dhclient enp0s8 -v


route add _gateway gw 0.0.0.0 enp0s8
route add default gw dev enp0s8
route add default _gateway enp0s8 
route add 192.168.1.254 gw _gateway enp0s8

ip route add _gateway dev enp0s8
ip route add 192.168.1.254 dev enp0s8

ip route add 0.0.0.0 via 192.168.1.254 enp

route add default gw dsl-router eth0

Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
10.0.3.0        0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s3

Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         _gateway        0.0.0.0         UG    100    0        0 enp0s8
10.0.3.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s8
_gateway        0.0.0.0         255.255.255.255 UH    100    0        0 enp0s8
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.1.254   _gateway        255.255.255.255 UGH   100    0        0 enp0s8
192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s3

```


https://containersolutions.github.io/runbooks/posts/kubernetes/dns-failures/

---
https://aws.amazon.com/premiumsupport/knowledge-center/eks-dns-failure/
Get the ClusterIP of your CoreDNS service:
kubectl get service kube-dns -n kube-system

Verify that then DNS endpoints are exposed and pointing to CoreDNS pods:
kubectl -n kube-system get endpoints kube-dns

Verify that the kube-proxy pod is working
kubectl logs -n kube-system --selector 'k8s-app=kube-proxy'


Connect to the application pod to troubleshoot the DNS issue
cat /etc/resolv.conf

To verify that your pod can use the default ClusterIP to resolve an internal domain, run the following command in the shell inside the pod:

nslookup kubernetes 10.96.0.10
# how to fix?
nslookup kubernetes 10.96.0.10
;; connection timed out; no servers could be reached

To verify that your pod can use the IP address of the CoreDNS pod to resolve directly, run the following commands in the shell inside the pod:
nslookup kubernetes 10.10.0.9
nslookup amazon.com 10.10.1.13

k exec -it ngx-dep-6796688696-bl7hr -- nslookup amazon.com 10.96.0.10

Get more detailed logs from CoreDNS pods for debugging
1. edit configmap
kubectl -n kube-system edit configmap coredns

2. Check if the CoreDNS logs are failing or getting any hits from the application pod:
kubectl logs --follow -n kube-system --selector 'k8s-app=kube-dns'
or
for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); \
do kubectl logs --namespace=kube-system $p; done


kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name
kubectl logs --namespace=kube-system pod/coredns-78d9d6b49-r6gdh



kubectl get pods -n kube-system | grep kube-proxy
kubectl logs kube-proxy-4pv6r --tail=5 -n kube-system
kubectl logs kube-proxy-cz47k --tail=5 -n kube-system
kubectl logs --follow -n kube-system --selector 'k8s-app=kube-proxy'


kubectl describe endpoints kube-dns --namespace=kube-system
kubectl get endpoints kube-dns -n kube-system

k exec -it ngx-dep-6796688696-bl7hr -- nc -vz mariadb-svc 3306
k exec -it mariadb-dep-5559cc9455-s99dq -- mysql -u wp -p

kubectl exec -ti ngx-dep-6796688696-mvm75 -- nc -vz 10.96.0.1 443
10.96.0.1 (10.96.0.1:443) open

kubectl exec -ti ngx-dep-6796688696-mvm75 -- nc -vz 10.96.0.10 53
nc: 10.96.0.10 (10.96.0.10:53): Host is unreachable
command terminated with exit code 1
sometimes it works
kubectl exec -ti ngx-dep-6796688696-mvm75 -- nc -vz 10.96.0.10 53
10.96.0.10 (10.96.0.10:53) open

kubectl exec -ti ngx-dep-6796688696-mvm75 -- nc -vz 10.10.0.10 53
nc: 10.10.0.10 (10.10.0.10:53): Host is unreachable
command terminated with exit code 1

kubectl exec -ti ngx-dep-6796688696-mvm75 -- nc -vz 10.10.1.17 53
10.10.1.17 (10.10.1.17:53) open


upgrade linux kernel
sudo apt update
$ sudo apt full-upgrade
$ reboot

sudo dpkg --list | egrep 'linux-image|linux-headers'

uname -r

#auto remove
sudo apt autoremove --purge
# manually remove old
sudo apt purge linux-image-5.8.0-50-generic


kubectl -n kube-system rollout restart deployment coredns
kubectl -n kube-flannel rollout restart ds kube-flannel-ds


https://www.anycodings.com/1questions/3090940/requests-timing-out-when-accesing-a-kubernetes-clusterip-service


https://github.com/flannel-io/flannel/blob/master/Documentation/configuration.md
https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml#L154


kubectl get po -A
kubernetes.default


kubectl edit cm -n kube-system kubeadm-config
kubectl edit cm -n kube-system kubelet-config

/var/lib/kubelet/config.conf
/var/lib/kubelet/kubeadm-flags.env
--hostname-override

systemctl daemon-reload
systemctl restart kubelet

kubectl edit no <node-name>

A kubelet restart will be required after changing /var/lib/kubelet/config.conf or /var/lib/kubelet/kubeadm-flags.env.






lsns 是一个用于列出主机上所有可用命名空间的命令。

lsns 只显示每个进程最小的 PID，但你可以根据这个进程 ID 进一步过滤。

网络命名空间可以通过 ip-netns 进行管理，使用 ip netns list 可以列出主机上的命名空间。

sudo lsns -t net
        NS TYPE NPROCS    PID USER     NETNSID NSFS                           COMMAND
4026531840 net     172      1 root  unassigned /run/docker/netns/default      /sbin/init
4026532317 net       2   3630 65535          0 /run/docker/netns/c21876ccef51 /pause
4026532418 net       2   3637 65535          1 /run/docker/netns/73c6b7072a34 /pause
4026532528 net       4 167850 65535          3 /run/docker/netns/110171b00c38 /pause
4026532625 net       4 167852 65535          2 /run/docker/netns/2fc0539f4c8a /pause


sudo lsns -p 168001
        NS TYPE   NPROCS    PID USER  COMMAND
4026531834 time      185      1 root  /sbin/init
4026531837 user      185      1 root  /sbin/init
4026532623 ipc         4 167852 65535 /pause
4026532625 net         4 167852 65535 /pause
4026532724 mnt         3 168001 root  nginx: master process nginx -g daemon off;
4026532725 uts         3 168001 root  nginx: master process nginx -g daemon off;
4026532726 pid         3 168001 root  nginx: master process nginx -g daemon off;
4026532727 cgroup      3 168001 root  nginx: master process nginx -g daemon off;


集群中的每个 Pod 都有一个额外的隐藏容器在后台运行，称为 pause 容器。

列出在节点上运行的容器并获取 pause 容器：

docker ps | grep pause
bb3295c76279   k8s.gcr.io/pause:3.6   "/pause"                 2 hours ago   Up 2 hours             k8s_POD_ngx-dep-bfbb5f64b-khzq9_default_45626e23-0056-4433-ba6a-a78d5b0f9b7a_0
73e169a1f7e3   k8s.gcr.io/pause:3.6   "/pause"                 2 hours ago   Up 2 hours             k8s_POD_ngx-dep-bfbb5f64b-znm8h_default_b747ab6a-bd76-4fcc-8170-79b430809a32_0
3b49bcb469d3   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_calico-kube-controllers-66966888c4-5p5b6_kube-system_a7f58ef0-4907-430f-a881-ff07ceb5901d_1
0263d977a82e   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_coredns-5768477497-tpdm6_kube-system_4010ea41-19d0-44fb-8a09-07ae77aa380d_265
5bc10846a1ce   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_kube-scheduler-vm1_kube-system_13bd247aa58e9e7dcc399111b6941126_8
157b53b8cbf7   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_calico-node-6mc6b_kube-system_44875cdb-e55c-460b-8976-aed66bf2f6f2_1
4f89439fa52c   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_kube-proxy-ntnbc_kube-system_d4230fce-52ba-479d-baf4-149800435aa2_8
ae7f5edb6e0f   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_kube-apiserver-vm1_kube-system_d9d359bd72a26a353f4810654216d31b_8
f20ccb86c626   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_kube-controller-manager-vm1_kube-system_9a1ea449428228019701983d923d3b08_8
e44f2ddd90da   k8s.gcr.io/pause:3.6   "/pause"                 5 hours ago   Up 5 hours             k8s_POD_etcd-vm1_kube-system_a5772e8efe3babae6494ff56820477e5_8

ip link show type veth
8: cali16f2d44a343@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0
9: calid7492f1f4c2@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1
20: caliae2a0ce1069@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2
21: cali94b9893d4de@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 3


默认情况下，在 Kubernetes 中创建 Service 时，被分配一个虚拟 IP。
Service 的虚拟 IP 保持静态不变，流量可以再无需干预的情况下，到达新创建的 Pod。
换句话说，Kubernetes 中的 Service 类似于负载均衡器。

Kubernetes 中的 Service 是基于 Linux 内核中的两个组件构建的：

    网络过滤器
    iptables

当提到 iptables 时，通常指的是 IPv4。对于 IPv6 ，终端工具是 ip6tables。

iptables 有五种链，每一种链都直接映射到 Netfilter 的钩子上。

从 iptables 的角度来看，它们是：

    PRE_ROUTING
    INPUT
    FORWARD
    OUTPUT
    POST_ROUTING

它们对应地映射到 Netfilter 钩子：

    NF_IP_PRE_ROUTING
    NF_IP_LOCAL_IN
    NF_IP_FORWARD
    NF_IP_LOCAL_OUT
    NF_IP_POST_ROUTING

当一个数据包到达时，根据它所处的阶段，将 “触发” 一个 Netfilter 钩子。这个钩子会执行特定的 iptables 过滤规则。


ip link show  type veth
8: cali16f2d44a343@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0
9: calid7492f1f4c2@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1
20: caliae2a0ce1069@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2
21: cali94b9893d4de@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 3
dao@vm1:~/kube/metrics$ ip link show type bridge
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:79:a0:21:2f brd ff:ff:ff:ff:ff:ff



run on master node
sudo kubeadm certs check-expiration
sudo kubeadm certs renew all

docker ps |egrep "k8s_kube-apiserver|k8s_kube-scheduler|k8s_kube-controller"|awk '{print $1}'|xargs docker restart


kubectl get events --sort-by=.metadata.creationTimestamp



if [ "$1" == "" ]
then
   echo "Usage: $0 hostname"
   echo "Example: $0 www.kaper.com"
   echo
   echo "The following hostnames are available:"
   echo
   kubectl get --all-namespaces ingress -o json | jq -r '.items[].spec.rules[].host' | sort -u
   exit 1
fi

export HOST=$1

(
    echo "HOST PATH NAMESPACE SERVICE PORT INGRESS REWRITE"
    echo "---- ---- --------- ------- ---- ------- -------"
    kubectl get --all-namespaces ingress -o json | \
        jq -r '.items[] | . as $parent | .spec.rules[] | select(.host==$ENV.HOST) | .host as $host | .http.paths[] | ( $host + " " + .path + " " + $parent.metadata.namespace + " " + .backend.service.name + " " + (.backend.service.port.number // .backend.service.port.name | tostring) + " " + $parent.metadata.name + " " + $parent.metadata.annotations."nginx.ingress.kubernetes.io/rewrite-target")' | \
        sort
) | column -s\  -t



      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $http_x_forwarded_host;

kubectl port-forward --namespace=nginx-ingress service/ngx-lb-svc 443:https

Bh914URJaAlApkm0*(

          # annotations:
  #   nginx.ingress.kubernetes.io/configuration-snippet: |
  #     more_clear_input_headers "Host" "X-Forwarded-Host";
  #     proxy_set_header Host $host:$server_port;
  #     proxy_set_header X-Forwarded-Host $host;
  #     proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;


kube_proxy_extra_args: ["--feature-gates=SupportIPVSProxyMode=true","--proxy-mode=ipvs"]

the Kubernetes Service is just a logical concept, the real work is being done by the “kube-proxy” pod that is running on each node.