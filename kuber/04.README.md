# 探索云原生

## 数据持久化

Kubernetes 管理存储资源的 API 对象 PersistentVolume 、 PersistentVolumeClaim 、 StorageClass 。

### Persistent Volume

Pod 里的容器是由镜像产生的，而镜像文件本身是只读的，进程要读写磁盘只能用一个临时的存储空间，一旦 Pod 销毁，临时存储也就会立即回收释放，数据也就丢失了。怎么办呢？

Kubernetes 的 Volume 是对数据存储的一个很好的抽象，它定义了有这么一个“存储卷”，而这个“存储卷”是什么类型、有多大容量、怎么存储，都可以自由发挥。Pod 不需要关心那些专业、复杂的细节，只要设置好 volumeMounts ，就可以把 Volume 加载进容器里使用。顺着 Volume 的概念， Kubernetes 延伸出了 PersistentVolume (PV) 对象，它专门用来表示持久存储设备，但隐藏了存储的底层实现。

作为存储的抽象， PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS，甚至是本地磁盘，管理它们已经超出了 Kubernetes 的能力范围，一般会由系统管理员单独维护，然后再在 Kubernetes 里创建对应的 PV。

>PV 属于集群的系统资源，是和 Node 平级的一种对象，Pod 对它没有管理权，只有使用权。

### PersistentVolumeClaim/StorageClass

有了 PV，是否就可以直接在 Pod 里挂载使用了呢？还不行。因为不同存储设备的差异实在是太大了。

于是 Kubernetes 就又增加了两个新对象： PersistentVolumeClaim (PVC) 和 StorageClass，用的还是“中间层”的思想，把存储卷的分配管理过程再次细化。
（简化 Pod 挂载“虚拟盘”的过程， Pod 看不到，也不关心 PV 的实现细节。）

- PersistentVolumeClaim，简称 PVC，从名字上看比较好理解，就是用来向 Kubernetes 申请存储资源的。  
  PVC 是给 Pod 使用的对象，它相当于是 Pod 的代理，代表 Pod 向系统申请 PV。一旦资源申请成功，Kubernetes 就会把 PV 和 PVC 关联在一起，这个动作叫做“绑定”（bind）。

- StorageClass 在 PVC 和 PV 之间充当“协调人”的角色，帮助 PVC 找到合适的 PV。  
  系统里的存储资源非常多，如果要 PVC 去直接遍历查找合适的 PV 也很麻烦，所以就要用到 StorageClass。
  它的作用有点像 IngressClass，它抽象了特定类型的存储系统，归类分组 PV 对象，用来简化 PV 和 PVC 的绑定过程。

<img alt="pv-pvc-storageclass" src="https://static001.geekbang.org/resource/image/5e/22/5e21d007a6152ec9594919300c2b6e22.jpg?wh=1920x1053" width="60%"/><br/>

### YAML 描述 PersistentVolume

Kubernetes 里有很多种类型的 PV，
- 最容易的本机存储 `HostPath`，它和 Docker 里挂载本地目录的 -v 参数非常类似，可以用它来初步认识一下 PV 的用法。

因为 Pod 会在集群的任意节点上运行，所以首先，我们要作为系统管理员在每个节点上创建一个目录，它将会作为本地存储卷挂载到 Pod 里。

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: host-10m-pv

spec:
  storageClassName: host-test
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Mi
  hostPath:
    path: /tmp/host-10m-pv/
```

解释：
- accessModes ：定义了存储设备的访问模式，简单来说就是虚拟盘的读写权限，和 Linux 的文件访问模式差不多，目前 Kubernetes 里有 3 种：
   - ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。
   - ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。
   - ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。

- capacity ：表示存储设备的容量，这里设置为 10MB。
  >Kubernetes 里定义存储容量使用的是国际标准， KB/MB/GB 的基数是 1024，要写成 Ki/Mi/Gi 。

- hostPath ：它指定了存储卷的本地路径，也就是在节点上的目录。

### YAML 描述 PersistentVolumeClaim

定义 PVC 对象，向 Kubernetes 申请存储。

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: host-5m-pvc

spec:
  storageClassName: host-test
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Mi
```

解释：

PVC 的内容与 PV 很像，但它不表示实际的存储，而是一个“申请”或者“声明”， spec 里的字段描述的是对存储的“期望状态”。

PVC 里的 storageClassName 、 accessModes 和 PV 是一样的，但不会有字段 capacity ，而是要用 resources.request 表示希望要有多大的容量。

### 使用 PersistentVolume

创建 PV

```shell
kubectl apply -f host-path-pv.yml
kubectl get pv
#
NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
host-10m-pv   10Mi       RWO            Retain           Available           host-test               5m22s
```

创建 PVC

```shell
kubectl apply -f host-path-pvc.yml
kubectl get pvc
#
NAME          STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE
host-5m-pvc   Bound    host-10m-pv   10Mi       RWO            host-test      4s
```

一旦 PVC 对象创建成功， Kubernetes 就会立即通过 StorageClass 、 resources 等条件在集群里查找符合要求的 PV ，如果找到合适的存储对象就会把它俩“绑定”在一起。  
此处 PVC 对象申请的是 5MB，现在系统里只有一个 10MB 的 PV，没有更合适的对象，所以 Kubernetes 也只能把这个 PV 分配出去，多出的容量就算是“福利”了。

如果把 PVC 的申请容量改大一些会怎么样呢？比如改成 100MB？  
会看到 PVC 会一直处于 Pending 状态，这意味着 Kubernetes 在系统里没有找到符合要求的存储，无法分配资源，只能等有满足要求的 PV 才能完成绑定。

### 为 Pod 挂载 PersistentVolume

PV 和 PVC 绑定好了，有了持久化存储，现在可以为 Pod 挂载存储卷。
先要在 `spec.volumes` 定义存储卷，然后在 `containers.volumeMounts` 挂载进容器。
因为用的是 PVC ，所以要在 volumes 里用字段 `persistentVolumeClaim` 指定 PVC 的名字。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: host-pvc-pod

spec:
  volumes:
  - name: host-pvc-vol
    persistentVolumeClaim:
      claimName: host-5m-pvc

  containers:
    - name: ngx-pvc-pod
      image: nginx:alpine
      ports:
      - containerPort: 80
      volumeMounts:
      - name: host-pvc-vol
        mountPath: /tmp
```

<img alt="pv-pvc-hostpath" src="https://static001.geekbang.org/resource/image/a4/d8/a4d709808a0ef729604c884c50748bd8.jpg?wh=1920x1310" width="50%"/><br/>
(Pod 、PVC 和 PV 的关系)

```shell
kubectl apply -f host-path-pod.yml
kubectl get pod -o wide
#
NAME           READY   STATUS    RESTARTS   AGE   IP            NODE   NOMINATED NODE   READINESS GATES
host-pvc-pod   1/1     Running   0          34s   10.244.1.34   vm2    <none>           <none>
```

可以看到 Pod 被分配到 vm2 节点上，那么 PV 是否确实挂载成功了呢？执行一些命令看看：

```bash
# check the path on bare host directory volume. run the command line on vm2
ll /tmp/
# 可以看到 volume /tmp/host-10m-pv 在 vm2 上自动生成了
drwxr-xr-x  2 root root 4096 Sep 15 14:50 host-10m-pv/

# go into pod
kubectl exec -it host-pvc-pod -- sh
/ # cd tmp
/tmp # echo hello world > a.txt
```

在 vm2 节点检查一下：

```bash
ll /tmp/host-10m-pv/
#
-rw-r--r--  1 root root   12 Sep 15 14:57 a.txt

cat /tmp/host-10m-pv/a.txt
#
hello world
```

可以看到确实在 vm2 节点的本地目录有一个 a.txt 的文件，再对一下时间，就可以确认是刚才在 Pod 里生成的文件。
因为 Pod 产生的数据已经通过 PV 存在了磁盘上，所以如果 Pod 删除后再重新创建，挂载存储卷时会依然使用这个目录，数据保持不变，也就实现了持久化存储。

不过因为这个 PV 是 HostPath 类型，只在本节点存储，如果 Pod 重建时被调度到了其他节点上，那么即使加载了本地目录，也不会是之前的存储位置，持久化功能也就失效了。

>**HostPath 类型的 PV 一般用来做测试。**

### 小结/补充

- Kubernets 有一种特殊形式的存储卷 `emptyDir`，它的生命周期与 Pod 相同，可以用于暂存或缓存。
- 如果存储系统符合 CSI (Container Storage Interface) 标准，那么 accessModes 可以设置成 ReadWriteOncePod ，只允许单个 Pod 读写。

## 使用网络共享卷

—— PersistentVolume + NFS (Network File System)

想让存储卷真正能被 Pod 任意挂载，需要变更存储的方式，不能限定在本地磁盘，而是要改成网络存储。这样 Pod 只要知道网络存储设备的 IP 地址或者域名，就可以通过网络通信访问。

网络存储是一个非常热门的应用领域，有很多知名的产品，比如 AWS、Azure、Ceph，Kubernetes 还专门定义了 CSI 规范，不过这些存储类型的安装、使用都比较复杂，在实验环境里部署难度比较高。此处以相对比较简单的 NFS 系统为例，来理解在 Kubernetes 里使用网络存储、静态存储卷和动态存储卷的概念。

### 安装 NFS 服务器

作为一个经典的网络存储系统，NFS 有着近 40 年的发展历史，基本上是各种 UNIX / Linux 系统的标准配置。

NFS 采用的是 Client/Server 架构，需要选定一台主机作为 Server，安装 NFS 服务端；其他要使用存储的主机作为 Client，安装 NFS 客户端工具。

此处以 Master 节点 vm1 作为 NFS 服务端。

```bash
sudo apt -y install nfs-kernel-server
```

安装好后，指定 NFS 的存储位置，这里使用了临时目录 /tmp/nfs

```bash
mkdir /tmp/nfs
```

接下来需要配置 NFS 访问共享目录，修改 `/etc/exports`，指定目录名、允许访问的网段，还有权限等参数。

```bash
vi /etc/exports
/tmp/nfs 192.168.56.1/24(rw,sync,no_subtree_check,no_root_squash,insecure)
```

让 NFS 配置生效：

```bash
sudo exportfs -ra
# 验证效果
sudo exportfs -v
#
/tmp/nfs      192.168.56.1/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,insecure,no_root_squash,no_all_squash)
```

启动 NFS 服务

```bash
sudo systemctl enable nfs-server
sudo systemctl start nfs-server
systemctl status nfs-server
```

使用命令 showmount 来检查 NFS 的网络挂载情况：

```bash
showmount -e 192.168.56.101
#
Export list for 192.168.56.101:
/tmp/nfs 192.168.56.1/24
```

### 安装 NFS 客户端

为了让 Kubernetes 集群能够访问 NFS 存储服务，需要在每个节点上都安装 NFS 客户端。

```bash
sudo apt -y install nfs-common
```

在节点上使用 showmount 检查 NFS 能否正常挂载：

```bash
showmount -e 192.168.56.101
```

尝试手动挂载 NFS 网络存储，将本地 /tmp/local 作为挂载点：

```bash
mkdir -p /tmp/nfs-local
sudo mount -t nfs 192.168.56.101:/tmp/nfs /tmp/nfs-local
# 1) 在 /tmp/test 里随便创建一个文件
cd /tmp/nfs-local/
echo hello-world > x.txt
# 2) 回到 NFS 服务器，检查共享目录 /tmp/nfs
ll /tmp/nfs/
#
-rw-rw-r--  1 dao  dao    12 Sep 15 15:50 x.txt
```

### 使用 NFS 存储卷

#### 创建 PV

先手工分配一个存储卷，需要指定 storageClassName 是 nfs ；再设置 accessModes 为 ReadWriteMany ， NFS 支持多个节点同时访问一个共享目录。

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-100m-pv

spec:
  storageClassName: nfs-test
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 100Mi

  nfs:
    server: 192.168.56.101
    path: /tmp/nfs/100m-pv
```

```bash
k apply -f pv/nfs-static-pv.yml
k get pv
#
NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                 STORAGECLASS   REASON   AGE
nfs-100m-pv   100Mi      RWX            Retain           Available                         nfs-test                6s
```

>注意：
>spec.nfs 里的 IP 地址一定要正确，路径目录一定要事先创建好！

#### 创建 PVC

它的内容和 PV 差不多，但不涉及 NFS 存储的细节，只需要用指定申请多大的容量。

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-static-pvc

spec:
  storageClassName: nfs-test
  accessModes:
  - ReadWriteMany

  resources:
    requests:
      storage: 100Mi
```

```bash
k apply -f pv/nfs-static-pvc.yml
persistentvolumeclaim/nfs-static-pvc created
k get pvc
#
NAME             STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-static-pvc   Bound    nfs-100m-pv   100Mi      RWX            nfs-test       6s
```

#### 创建 Pod

```yml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-static-pod

spec:
  volumes:
  - name: nfs-static-vol
    persistentVolumeClaim:
      claimName: nfs-static-pvc

  containers:
    - name: ngx-nfs-static-pod
      image: nginx:alpine
      ports:
      - containerPort: 80
    
      volumeMounts:
      - name: nfs-static-vol
        mountPath: /tmp
```

```bash
k apply -f pv/nfs-static-pod.yml
k get pod -o wide
#
NAME             READY   STATUS    RESTARTS      AGE   IP            NODE   NOMINATED NODE   READINESS GATES
nfs-static-pod   1/1     Running   0             11s   10.244.1.37   vm2    <none>           <none>
```

<img alt="pv-pvc-nfs" src="https://static001.geekbang.org/resource/image/2a/a7/2a21d16b028afdea4f525439bd8f06a7.jpg?wh=1920x1125" width="50%"/><br/>
(Pod、PVC、PV 和 NFS 存储的关系)

#### 检验

Kubernetes 会自动执行 NFS 挂载动作，把 NFS 的共享目录 /tmp/nfs/100m-pv 挂载到 Pod 里的 /tmp 。

进入 Pod ，试着操作 NFS 共享目录：

```bash
# vm2
k exec -it nfs-static-pod -- sh
cd /tmp/
echo hello-world > a.txt

# vm1 (NFS Server)
ll /tmp/nfs/100m-pv/
-rw-r--r-- 1 root root   12 Sep 16 02:50 a.txt

cat /tmp/nfs/100m-pv/a.txt
hello-world
```

### 部署 NFS Provisoner

有了 NFS 网络存储系统，是否 Kubernetes 里的数据持久化问题就已经解决了？
“解决了，但没有完全解决。”

因为 PV 还是需要人工管理，必须要由系统管理员手动维护各种存储设备，再根据开发需求逐个创建 PV，而且 PV 的大小也很难精确控制，容易出现空间不足或者空间浪费的情况。

那么能不能让创建 PV 的工作也实现自动化呢？让计算机来代替人类来分配存储卷呢？这时“动态存储卷”的概念呼之欲出，它利用 StorageClass 绑定一个 Provisioner 对象，这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。

> Provisioner 就是 [NFS subdir external provisioner - GitHub](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)

NFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，它所需的 YAML 文件，一共有三个：
- rbac.yaml
  
  需要修改它使用的是默认的 default 名字空间，改成其他的名字空间，避免与普通应用混在一起，可以用“查找替换”的方式把它统一改成 kube-system 。[参考](./src/pv/nfs-provisioner/rbac.yml)

- class.yaml

- deployment.yaml

  主要修改 名字空间 和 NFS 配置：
  首先，要把名字空间改成和 rbac.yaml 一样，比如 kube-system ；
  然后，要修改配置到集群里的 NFS 服务器，比如， volumes 和 env 里的 IP 地址和共享目录名。
  [参考](./src/pv/nfs-provisioner/deployment.yml)

```bash
k apply -f nfs-provisioner/rbac.yml
#
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created

k apply -f nfs-provisioner/class.yml
#
storageclass.storage.k8s.io/nfs-client created

k apply -f nfs-provisioner/deployment.yml
#
deployment.apps/nfs-client-provisioner created

k get deploy -A
#
NAMESPACE     NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   nfs-client-provisioner   1/1     1            1           20s
```

### 使用 NFS 动态存储卷

查看 NFS 默认的 StorageClass 定义，[参考](./src/pv/nfs-provisioner/class.yml)，

YAML 里的关键字段是 provisioner ：它指定了应该使用哪个 Provisioner。另一个字段 parameters 是调节 Provisioner 运行的参数，需要[参考文档](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/tree/v4.0.2#build-and-publish-your-own-container-image)来确定具体值，在这里的 archiveOnDelete: "false" 就是自动回收存储空间。

NFS 动态卷我们可以根据自己的需求，任意定制具有不同存储特性的 StorageClass ，比如添加字段 onDelete: "retain" 暂时保留分配的存储，之后再手动删除。

>OnDelete: If it exists and has a delete value, delete the directory, if it exists and has a retain value, save the directory.

```yml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client-retained

provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  onDelete: "retain"
```

#### 实践

接下来定义一个 PVC ，向系统申请 10MB 的存储空间，使用的 StorageClass 是默认的 nfs-client ：

pvc

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-dyn-10m-pvc

spec:
  storageClassName: nfs-client
  accessModes:
  - ReadWriteMany

  resources:
    requests:
      storage: 10Mi
```

pod

```yml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-dyn-pod

spec:
  volumes:
  - name: nfs-dyn-vol
    persistentVolumeClaim:
      claimName: nfs-dyn-10m-pvc

  containers:
    - name: ngx-nfs-dyn-pod
      image: nginx:alpine
      ports:
      - containerPort: 80
    
      volumeMounts:
      - name: nfs-dyn-vol
        mountPath: /tmp
```

检验

```bash
k apply -f nfs-dyn-pvc.yml
persistentvolumeclaim/nfs-dyn-10m-pvc created

ll /tmp/nfs/
#
drwxrwxrwx  2 root root 4096 Sep 16 04:49 default-nfs-dyn-10m-pvc-pvc-588e6c06-9d21-491d-baec-9a6d35635c2b/

k apply -f nfs-dyn-pod.yml
pod/nfs-dyn-pod created

k get pod
#
NAME             READY   STATUS    RESTARTS   AGE
nfs-dyn-pod      1/1     Running   0          5s

k get pv
#
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
pvc-588e6c06-9d21-491d-baec-9a6d35635c2b   10Mi       RWX            Delete           Bound    default/nfs-dyn-10m-pvc   nfs-client              2m

k get pvc
#
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-dyn-10m-pvc   Bound    pvc-588e6c06-9d21-491d-baec-9a6d35635c2b   10Mi       RWX            nfs-client     2m2s
```

>我们不需要定义 PV 对象，有 NFS Provisioner 自动创建一个 PV ，大小刚好是在 PVC 里申请的 10MB 。

<img alt="pv-pvc-provisioner" src="https://static001.geekbang.org/resource/image/e3/1e/e3905990be6fb8739fb51a4ab9856f1e.jpg?wh=1920x856" width="70%"/><br/>
(Pod、PVC、StorageClass 和 Provisioner 的关系)

### 小结

- 在 Kubernetes 集群里，网络存储系统更适合数据持久化，NFS 是最容易使用的一种网络存储系统，要事先安装好服务端和客户端。
- 可以编写 PV 手工定义 NFS 静态存储卷，要指定 NFS 服务器的 IP 地址和共享目录名。
- 使用 NFS 动态存储卷必须要部署相应的 Provisioner ，在 YAML 里正确配置 NFS 服务器。
- 动态存储卷不需要手工定义 PV，而是要定义 StorageClass，由关联的 Provisioner 自动创建 PV 完成绑定。
