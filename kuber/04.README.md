# 探索云原生

## 数据持久化

Kubernetes 管理存储资源的 API 对象 PersistentVolume 、 PersistentVolumeClaim 、 StorageClass 。

### Persistent Volume

Pod 里的容器是由镜像产生的，而镜像文件本身是只读的，进程要读写磁盘只能用一个临时的存储空间，一旦 Pod 销毁，临时存储也就会立即回收释放，数据也就丢失了。怎么办呢？

Kubernetes 的 Volume 是对数据存储的一个很好的抽象，它定义了有这么一个“存储卷”，而这个“存储卷”是什么类型、有多大容量、怎么存储，都可以自由发挥。Pod 不需要关心那些专业、复杂的细节，只要设置好 volumeMounts ，就可以把 Volume 加载进容器里使用。顺着 Volume 的概念， Kubernetes 延伸出了 PersistentVolume (PV) 对象，它专门用来表示持久存储设备，但隐藏了存储的底层实现。

作为存储的抽象， PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS，甚至是本地磁盘，管理它们已经超出了 Kubernetes 的能力范围，一般会由系统管理员单独维护，然后再在 Kubernetes 里创建对应的 PV。

>PV 属于集群的系统资源，是和 Node 平级的一种对象，Pod 对它没有管理权，只有使用权。

### PersistentVolumeClaim/StorageClass

有了 PV，是否就可以直接在 Pod 里挂载使用了呢？还不行。因为不同存储设备的差异实在是太大了。

于是 Kubernetes 就又增加了两个新对象： PersistentVolumeClaim (PVC) 和 StorageClass，用的还是“中间层”的思想，把存储卷的分配管理过程再次细化。
（简化 Pod 挂载“虚拟盘”的过程， Pod 看不到，也不关心 PV 的实现细节。）

- PersistentVolumeClaim，简称 PVC，从名字上看比较好理解，就是用来向 Kubernetes 申请存储资源的。  
  PVC 是给 Pod 使用的对象，它相当于是 Pod 的代理，代表 Pod 向系统申请 PV。一旦资源申请成功，Kubernetes 就会把 PV 和 PVC 关联在一起，这个动作叫做“绑定”（bind）。

- StorageClass 在 PVC 和 PV 之间充当“协调人”的角色，帮助 PVC 找到合适的 PV。  
  系统里的存储资源非常多，如果要 PVC 去直接遍历查找合适的 PV 也很麻烦，所以就要用到 StorageClass。
  它的作用有点像 IngressClass，它抽象了特定类型的存储系统，归类分组 PV 对象，用来简化 PV 和 PVC 的绑定过程。

<img alt="pv-pvc-storageclass" src="https://static001.geekbang.org/resource/image/5e/22/5e21d007a6152ec9594919300c2b6e22.jpg?wh=1920x1053" width="60%"/><br/>

### YAML 描述 PersistentVolume

Kubernetes 里有很多种类型的 PV，
- 最容易的本机存储 `HostPath`，它和 Docker 里挂载本地目录的 -v 参数非常类似，可以用它来初步认识一下 PV 的用法。

因为 Pod 会在集群的任意节点上运行，所以首先，我们要作为系统管理员在每个节点上创建一个目录，它将会作为本地存储卷挂载到 Pod 里。

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: host-10m-pv

spec:
  storageClassName: host-test
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Mi
  hostPath:
    path: /tmp/host-10m-pv/
```

解释：
- accessModes ：定义了存储设备的访问模式，简单来说就是虚拟盘的读写权限，和 Linux 的文件访问模式差不多，目前 Kubernetes 里有 3 种：
   - ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。
   - ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。
   - ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。

- capacity ：表示存储设备的容量，这里设置为 10MB。
  >Kubernetes 里定义存储容量使用的是国际标准， KB/MB/GB 的基数是 1024，要写成 Ki/Mi/Gi 。

- hostPath ：它指定了存储卷的本地路径，也就是在节点上的目录。

### YAML 描述 PersistentVolumeClaim

定义 PVC 对象，向 Kubernetes 申请存储。

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: host-5m-pvc

spec:
  storageClassName: host-test
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Mi
```

解释：

PVC 的内容与 PV 很像，但它不表示实际的存储，而是一个“申请”或者“声明”， spec 里的字段描述的是对存储的“期望状态”。

PVC 里的 storageClassName 、 accessModes 和 PV 是一样的，但不会有字段 capacity ，而是要用 resources.request 表示希望要有多大的容量。

### 使用 PersistentVolume

创建 PV

```shell
kubectl apply -f host-path-pv.yml
kubectl get pv
#
NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
host-10m-pv   10Mi       RWO            Retain           Available           host-test               5m22s
```

创建 PVC

```shell
kubectl apply -f host-path-pvc.yml
kubectl get pvc
#
NAME          STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE
host-5m-pvc   Bound    host-10m-pv   10Mi       RWO            host-test      4s
```

一旦 PVC 对象创建成功， Kubernetes 就会立即通过 StorageClass 、 resources 等条件在集群里查找符合要求的 PV ，如果找到合适的存储对象就会把它俩“绑定”在一起。  
此处 PVC 对象申请的是 5MB，现在系统里只有一个 10MB 的 PV，没有更合适的对象，所以 Kubernetes 也只能把这个 PV 分配出去，多出的容量就算是“福利”了。

如果把 PVC 的申请容量改大一些会怎么样呢？比如改成 100MB？  
会看到 PVC 会一直处于 Pending 状态，这意味着 Kubernetes 在系统里没有找到符合要求的存储，无法分配资源，只能等有满足要求的 PV 才能完成绑定。

### 为 Pod 挂载 PersistentVolume

PV 和 PVC 绑定好了，有了持久化存储，现在可以为 Pod 挂载存储卷。
先要在 `spec.volumes` 定义存储卷，然后在 `containers.volumeMounts` 挂载进容器。
因为用的是 PVC ，所以要在 volumes 里用字段 `persistentVolumeClaim` 指定 PVC 的名字。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: host-pvc-pod

spec:
  volumes:
  - name: host-pvc-vol
    persistentVolumeClaim:
      claimName: host-5m-pvc

  containers:
    - name: ngx-pvc-pod
      image: nginx:alpine
      ports:
      - containerPort: 80
      volumeMounts:
      - name: host-pvc-vol
        mountPath: /tmp
```

<img alt="pv-pvc-hostpath" src="https://static001.geekbang.org/resource/image/a4/d8/a4d709808a0ef729604c884c50748bd8.jpg?wh=1920x1310" width="50%"/><br/>
(Pod 、PVC 和 PV 的关系)

```shell
kubectl apply -f host-path-pod.yml
kubectl get pod -o wide
#
NAME           READY   STATUS    RESTARTS   AGE   IP            NODE   NOMINATED NODE   READINESS GATES
host-pvc-pod   1/1     Running   0          34s   10.244.1.34   vm2    <none>           <none>
```

可以看到 Pod 被分配到 vm2 节点上，那么 PV 是否确实挂载成功了呢？执行一些命令看看：

```bash
# check the path on bare host directory volume. run the command line on vm2
ll /tmp/
# 可以看到 volume /tmp/host-10m-pv 在 vm2 上自动生成了
drwxr-xr-x  2 root root 4096 Sep 15 14:50 host-10m-pv/

# go into pod
kubectl exec -it host-pvc-pod -- sh
/ # cd tmp
/tmp # echo hello world > a.txt
```

在 vm2 节点检查一下：

```bash
ll /tmp/host-10m-pv/
#
-rw-r--r--  1 root root   12 Sep 15 14:57 a.txt

cat /tmp/host-10m-pv/a.txt
#
hello world
```

可以看到确实在 vm2 节点的本地目录有一个 a.txt 的文件，再对一下时间，就可以确认是刚才在 Pod 里生成的文件。
因为 Pod 产生的数据已经通过 PV 存在了磁盘上，所以如果 Pod 删除后再重新创建，挂载存储卷时会依然使用这个目录，数据保持不变，也就实现了持久化存储。

不过因为这个 PV 是 HostPath 类型，只在本节点存储，如果 Pod 重建时被调度到了其他节点上，那么即使加载了本地目录，也不会是之前的存储位置，持久化功能也就失效了。

>**HostPath 类型的 PV 一般用来做测试。**

### 小结/补充

- Kubernets 有一种特殊形式的存储卷 `emptyDir`，它的生命周期与 Pod 相同，可以用于暂存或缓存。
- 如果存储系统符合 CSI (Container Storage Interface) 标准，那么 accessModes 可以设置成 ReadWriteOncePod ，只允许单个 Pod 读写。

## 使用网络共享卷

—— PersistentVolume + NFS (Network File System)

想让存储卷真正能被 Pod 任意挂载，需要变更存储的方式，不能限定在本地磁盘，而是要改成网络存储。这样 Pod 只要知道网络存储设备的 IP 地址或者域名，就可以通过网络通信访问。

网络存储是一个非常热门的应用领域，有很多知名的产品，比如 AWS、Azure、Ceph，Kubernetes 还专门定义了 CSI 规范，不过这些存储类型的安装、使用都比较复杂，在实验环境里部署难度比较高。此处以相对比较简单的 NFS 系统为例，来理解在 Kubernetes 里使用网络存储、静态存储卷和动态存储卷的概念。

### 安装 NFS 服务器

作为一个经典的网络存储系统，NFS 有着近 40 年的发展历史，基本上是各种 UNIX / Linux 系统的标准配置。

NFS 采用的是 Client/Server 架构，需要选定一台主机作为 Server，安装 NFS 服务端；其他要使用存储的主机作为 Client，安装 NFS 客户端工具。

此处以 Master 节点 vm1 作为 NFS 服务端。

```bash
sudo apt -y install nfs-kernel-server
```

安装好后，指定 NFS 的存储位置，这里使用了临时目录 /tmp/nfs

```bash
mkdir /tmp/nfs
```

接下来需要配置 NFS 访问共享目录，修改 `/etc/exports`，指定目录名、允许访问的网段，还有权限等参数。

```bash
vi /etc/exports
/tmp/nfs 192.168.56.1/24(rw,sync,no_subtree_check,no_root_squash,insecure)
```

让 NFS 配置生效：

```bash
sudo exportfs -ra
# 验证效果
sudo exportfs -v
#
/tmp/nfs      192.168.56.1/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,insecure,no_root_squash,no_all_squash)
```

启动 NFS 服务

```bash
sudo systemctl enable nfs-server
sudo systemctl start nfs-server
systemctl status nfs-server
```

使用命令 showmount 来检查 NFS 的网络挂载情况：

```bash
showmount -e 192.168.56.101
#
Export list for 192.168.56.101:
/tmp/nfs 192.168.56.1/24
```

### 安装 NFS 客户端

为了让 Kubernetes 集群能够访问 NFS 存储服务，需要在每个节点上都安装 NFS 客户端。

```bash
sudo apt -y install nfs-common
```

在节点上使用 showmount 检查 NFS 能否正常挂载：

```bash
showmount -e 192.168.56.101
```

尝试手动挂载 NFS 网络存储，将本地 /tmp/local 作为挂载点：

```bash
mkdir -p /tmp/nfs-local
sudo mount -t nfs 192.168.56.101:/tmp/nfs /tmp/nfs-local
# 1) 在 /tmp/test 里随便创建一个文件
cd /tmp/nfs-local/
echo hello-world > x.txt
# 2) 回到 NFS 服务器，检查共享目录 /tmp/nfs
ll /tmp/nfs/
#
-rw-rw-r--  1 dao  dao    12 Sep 15 15:50 x.txt
```

### 使用 NFS 存储卷

#### 创建 PV

先手工分配一个存储卷，需要指定 storageClassName 是 nfs ；再设置 accessModes 为 ReadWriteMany ， NFS 支持多个节点同时访问一个共享目录。

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-100m-pv

spec:
  storageClassName: nfs-test
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 100Mi

  nfs:
    server: 192.168.56.101
    path: /tmp/nfs/100m-pv
```

```bash
k apply -f pv/nfs-static-pv.yml
k get pv
#
NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                 STORAGECLASS   REASON   AGE
nfs-100m-pv   100Mi      RWX            Retain           Available                         nfs-test                6s
```

>注意：
>spec.nfs 里的 IP 地址一定要正确，路径目录一定要事先创建好！

#### 创建 PVC

它的内容和 PV 差不多，但不涉及 NFS 存储的细节，只需要用指定申请多大的容量。

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-static-pvc

spec:
  storageClassName: nfs-test
  accessModes:
  - ReadWriteMany

  resources:
    requests:
      storage: 100Mi
```

```bash
k apply -f pv/nfs-static-pvc.yml
persistentvolumeclaim/nfs-static-pvc created
k get pvc
#
NAME             STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-static-pvc   Bound    nfs-100m-pv   100Mi      RWX            nfs-test       6s
```

#### 创建 Pod

```yml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-static-pod

spec:
  volumes:
  - name: nfs-static-vol
    persistentVolumeClaim:
      claimName: nfs-static-pvc

  containers:
    - name: ngx-nfs-static-pod
      image: nginx:alpine
      ports:
      - containerPort: 80
    
      volumeMounts:
      - name: nfs-static-vol
        mountPath: /tmp
```

```bash
k apply -f pv/nfs-static-pod.yml
k get pod -o wide
#
NAME             READY   STATUS    RESTARTS      AGE   IP            NODE   NOMINATED NODE   READINESS GATES
nfs-static-pod   1/1     Running   0             11s   10.244.1.37   vm2    <none>           <none>
```

<img alt="pv-pvc-nfs" src="https://static001.geekbang.org/resource/image/2a/a7/2a21d16b028afdea4f525439bd8f06a7.jpg?wh=1920x1125" width="50%"/><br/>
(Pod、PVC、PV 和 NFS 存储的关系)

#### 检验

Kubernetes 会自动执行 NFS 挂载动作，把 NFS 的共享目录 /tmp/nfs/100m-pv 挂载到 Pod 里的 /tmp 。

进入 Pod ，试着操作 NFS 共享目录：

```bash
# vm2
k exec -it nfs-static-pod -- sh
cd /tmp/
echo hello-world > a.txt

# vm1 (NFS Server)
ll /tmp/nfs/100m-pv/
-rw-r--r-- 1 root root   12 Sep 16 02:50 a.txt

cat /tmp/nfs/100m-pv/a.txt
hello-world
```

### 部署 NFS Provisoner

有了 NFS 网络存储系统，是否 Kubernetes 里的数据持久化问题就已经解决了？
“解决了，但没有完全解决。”

因为 PV 还是需要人工管理，必须要由系统管理员手动维护各种存储设备，再根据开发需求逐个创建 PV，而且 PV 的大小也很难精确控制，容易出现空间不足或者空间浪费的情况。

那么能不能让创建 PV 的工作也实现自动化呢？让计算机来代替人类来分配存储卷呢？这时“动态存储卷”的概念呼之欲出，它利用 StorageClass 绑定一个 Provisioner 对象，这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。

> Provisioner 就是 [NFS subdir external provisioner - GitHub](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)

NFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，它所需的 YAML 文件，一共有三个：
- rbac.yaml
  
  需要修改它使用的是默认的 default 名字空间，改成其他的名字空间，避免与普通应用混在一起，可以用“查找替换”的方式把它统一改成 kube-system 。[参考](./src/pv/nfs-provisioner/rbac.yml)

- class.yaml

- deployment.yaml

  主要修改 名字空间 和 NFS 配置：
  首先，要把名字空间改成和 rbac.yaml 一样，比如 kube-system ；
  然后，要修改配置到集群里的 NFS 服务器，比如， volumes 和 env 里的 IP 地址和共享目录名。
  [参考](./src/pv/nfs-provisioner/deployment.yml)

```bash
k apply -f nfs-provisioner/rbac.yml
#
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created

k apply -f nfs-provisioner/class.yml
#
storageclass.storage.k8s.io/nfs-client created

k apply -f nfs-provisioner/deployment.yml
#
deployment.apps/nfs-client-provisioner created

k get deploy -A
#
NAMESPACE     NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   nfs-client-provisioner   1/1     1            1           20s
```

### 使用 NFS 动态存储卷

查看 NFS 默认的 StorageClass 定义，[参考](./src/pv/nfs-provisioner/class.yml)，

YAML 里的关键字段是 provisioner ：它指定了应该使用哪个 Provisioner。另一个字段 parameters 是调节 Provisioner 运行的参数，需要[参考文档](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/tree/v4.0.2#build-and-publish-your-own-container-image)来确定具体值，在这里的 archiveOnDelete: "false" 就是自动回收存储空间。

NFS 动态卷我们可以根据自己的需求，任意定制具有不同存储特性的 StorageClass ，比如添加字段 onDelete: "retain" 暂时保留分配的存储，之后再手动删除。

>OnDelete: If it exists and has a delete value, delete the directory, if it exists and has a retain value, save the directory.

```yml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client-retained

provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  onDelete: "retain"
```

#### 实践

接下来定义一个 PVC ，向系统申请 10MB 的存储空间，使用的 StorageClass 是默认的 nfs-client ：

pvc

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-dyn-10m-pvc

spec:
  storageClassName: nfs-client
  accessModes:
  - ReadWriteMany

  resources:
    requests:
      storage: 10Mi
```

pod

```yml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-dyn-pod

spec:
  volumes:
  - name: nfs-dyn-vol
    persistentVolumeClaim:
      claimName: nfs-dyn-10m-pvc

  containers:
    - name: ngx-nfs-dyn-pod
      image: nginx:alpine
      ports:
      - containerPort: 80
    
      volumeMounts:
      - name: nfs-dyn-vol
        mountPath: /tmp
```

检验

```bash
k apply -f nfs-dyn-pvc.yml
persistentvolumeclaim/nfs-dyn-10m-pvc created

ll /tmp/nfs/
#
drwxrwxrwx  2 root root 4096 Sep 16 04:49 default-nfs-dyn-10m-pvc-pvc-588e6c06-9d21-491d-baec-9a6d35635c2b/

k apply -f nfs-dyn-pod.yml
pod/nfs-dyn-pod created

k get pod
#
NAME             READY   STATUS    RESTARTS   AGE
nfs-dyn-pod      1/1     Running   0          5s

k get pv
#
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
pvc-588e6c06-9d21-491d-baec-9a6d35635c2b   10Mi       RWX            Delete           Bound    default/nfs-dyn-10m-pvc   nfs-client              2m

k get pvc
#
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-dyn-10m-pvc   Bound    pvc-588e6c06-9d21-491d-baec-9a6d35635c2b   10Mi       RWX            nfs-client     2m2s
```

>我们不需要定义 PV 对象，有 NFS Provisioner 自动创建一个 PV ，大小刚好是在 PVC 里申请的 10MB 。

<img alt="pv-pvc-provisioner" src="https://static001.geekbang.org/resource/image/e3/1e/e3905990be6fb8739fb51a4ab9856f1e.jpg?wh=1920x856" width="70%"/><br/>
(Pod、PVC、StorageClass 和 Provisioner 的关系)

### 小结

- 在 Kubernetes 集群里，网络存储系统更适合数据持久化，NFS 是最容易使用的一种网络存储系统，要事先安装好服务端和客户端。
- 可以编写 PV 手工定义 NFS 静态存储卷，要指定 NFS 服务器的 IP 地址和共享目录名。
- 使用 NFS 动态存储卷必须要部署相应的 Provisioner ，在 YAML 里正确配置 NFS 服务器。
- 动态存储卷不需要手工定义 PV，而是要定义 StorageClass，由关联的 Provisioner 自动创建 PV 完成绑定。
- NFS 最初是由 Sun 公司在1984年发布的，当前由 IETF 负责维护，最新版本已是4.2了。

## StatefulSet

Deployment 和 DaemonSet 是在 Kubernetes 集群里部署应用的重要工具，不过它们只能管理“无状态应用”（Stateless Application），不能管理“有状态应用”（Stateful Application）。

### 什么是有状态的应用

先从 PersistentVolume 谈起，有了持久化存储，应用就可以把一些运行时的关键数据落盘，相当于有了一份“保险”，如果 Pod 发生意外崩溃，也只不过像是按下了暂停键，等重启后挂载 Volume 后，再加载原数据就能够满血复活，恢复之前的“状态”继续运行。

有一些应用的状态信息不是很重要，即使不恢复状态也能够正常运行，这就是“无状态应用”。典型的例子如 Nginx ，它只是处理 HTTP 请求，本身不生产数据（日志除外），不需要特意保存状态，无论以什么状态重启都能很好地对外提供服务。

还有一些应用，运行状态信息很重要，如果因为重启而丢失了状态是绝对无法接受的，这就是“有状态应用”。例子很多，比如 Redis 、 MySQL 这样的数据库，它们的“状态”就是在内存或者磁盘上产生的数据，是应用的核心价值所在。

那么，是不是 Deployment 加上 PersistentVolume 就可以轻松管理有状态的应用呢？
的确，用 Deployment 来保证高可用，用 PersistentVolume 来存储数据，确实可以**部分达到**管理“有状态应用”的目的。

但是 Kubernetes 的眼光则更加全面和长远，它认为“状态”不仅仅是数据持久化，在集群化、分布式的场景里，还有多实例的*依赖关系*、*启动顺序*和*网络标识*等问题需要解决，而这些问题恰恰是 Deployment 力所不及的。

比如，对*有状态应用* ，多个实例之间可能存在依赖关系，比如 master/slave、active/passive，需要依次启动才能保证应用正常运行，外界的客户端也可能要使用固定的网络标识来访问实例，并且这些信息还必须要保证在 Pod 重启后不变。

### 描述 StatefulSet

用命令查看 StatefulSet 的基本信息：

```bash
kubectl api-resources | grep statefulset
#
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
statefulsets                      sts          apps/v1                                true         StatefulSet

```

=>构造出 YAML 文件头信息如下：

```yml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: xxx-sts # 它的简称是 sts
```

和 DaemonSet 类似， StatefulSet 也可以看做是 Deployment 的一个特例，它也不能直接用 kubectl create 创建样板文件，但可以简单修改 Deployment 得到。下面以 redis 为例：

```yaml
# k create deploy redis-sts --image=redis:alpine --port=6379 $out
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-sts

spec:
  serviceName: redis-svc  # 新增字段
  replicas: 2
  selector:
    matchLabels:
      app: redis-sts

  template:
    metadata:
      labels:
        app: redis-sts
    spec:
      containers:
      - image: redis:alpine
        name: redis
        ports:
        - containerPort: 6379
```

观察 YAML 文件，除了 kind 不同于 Deployment 外，在 spec 里多出了一个字段`serviceName`。

### 使用 StatefulSet

使用 StatefulSet 来解决上面说的*有状态应用*的三个问题。

#### 第一问题：启动顺序

执行上面的 redis-sts 观察结果：

```bash
k apply -f statefulset/redis-sts.yml
k get sts
#
NAME        READY   AGE
redis-sts   0/2     6s

k get pod
#
NAME             READY   STATUS    RESTARTS   AGE
redis-sts-0      1/1     Running   0          51s
redis-sts-1      1/1     Running   0          29s
```

可以看到，StatefulSet 所管理的 Pod 不再是随机的名字了，而是有了顺序编号，从 0 开始分别被命名为 redis-sts-0、redis-sts-1，Kubernetes 也会按照这个顺序依次创建（0 号比 1 号的 AGE 要长一点）。——解决了启动顺序的问题。

#### 第二个问题：依赖关系

接下来看，应用怎么知道自己的身份，进而确定互相之间的依赖关系呢？ Kubernetes 给出的方法是使用 hostname ，也就是每个 Pod 里的主机名。去 Pod 里看看：

```bash
k exec -it redis-sts-0 -- hostname
#
redis-sts-0

k exec -it redis-sts-1 -- cat /etc/hostname
#
redis-sts-1
```

有了这个唯一的名字，应用就可以自行决定依赖关系了，比如在这个 Redis 例子里，可以让先启动的 0 号 Pod 是主实例，后启动的 1 号 Pod 是从实例。

#### 第三个问题：网络标识

这就需要用到 Service 对象。

>这里不能用命令 kubectl expose 直接为 StatefulSet 生成 Service，需要手动编写 YAML 。

```yml
# kubectl create service clusterip redis-svc --tcp=6379 $out
apiVersion: v1
kind: Service
metadata:
  name: redis-svc # 指定的 service name 必须与 StatefulSet serviceName 相同

spec:
  selector:
    app: redis-sts  # 指定 StatefulSet name

  type: ClusterIP
  ports:
  - port: 6379
    protocol: TCP
    targetPort: 6379
```

查看结果：

```bash
k apply -f statefulset/redis-svc.yml
k get svc
#
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
redis-svc    ClusterIP   10.109.31.223   <none>        6379/TCP   4s

k describe svc redis-svc
#
Name:              redis-svc
Namespace:         default
Selector:          app=redis-sts
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.109.31.223
IPs:               10.109.31.223
Port:              <unset>  6379/TCP
TargetPort:        6379/TCP
Endpoints:         10.244.1.40:6379,10.244.1.41:6379
...
```

我们知道 Service 有一个域名，格式是`对象名.名字空间`，每个 Pod 也有一个域名，形式是`IP 地址.名字空间`。但因为 IP 地址不稳定，所以 Pod 的域名并不实用。

但 Service 对象应用于 StatefulSet 的时候，情况就不一样了。Service 发现这些 Pod 不是一般的应用，而是有状态应用，需要有稳定的网络标识，所以就会为 Pod 再多创建出一个新的域名，格式是`Pod 名.服务名.名字空间.svc.cluster.local`。也可以简写成`Pod 名.服务名`。
进入 Pod 内部，用 ping 命令来验证一下：

```bash
k exec -it redis-sts-0 -- ping redis-sts-1.redis-svc
#
PING redis-sts-1.redis-svc (10.244.1.41): 56 data bytes
64 bytes from 10.244.1.41: seq=0 ttl=64 time=0.076 ms
...

k exec -it redis-sts-1 -- ping redis-sts-0.redis-svc
#
PING redis-sts-0.redis-svc (10.244.1.40): 56 data bytes
64 bytes from 10.244.1.40: seq=0 ttl=64 time=0.132 ms
...
```

通过 StatefulSet 和 Service 的联合使用，就有了稳定的网络标识。

<img alt="statefulset-service" src="https://static001.geekbang.org/resource/image/49/22/490d814cf0f25db56537a20f3af57e22.jpg?wh=1920x1094" width="60%"/><br/>
(StatefulSet 与 Service 对象的关系)

### 实现 StatefulSet 的数据持久化

为了强调持久化存储与 StatefulSet 的一对一绑定关系，Kubernetes 为 StatefulSet 专门定义了一个字段`volumeClaimTemplates`，直接把 PVC 定义嵌入 StatefulSet 的 YAML 文件里。这样能保证创建 StatefulSet 的同时，就会为每个 Pod 自动创建 PVC，让 StatefulSet 的可用性更高。

```yml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-pv-sts

spec:
  serviceName: redis-svc

  volumeClaimTemplates: 
  - metadata: 
      name: redis-100m-pvc
    spec: 
      storageClassName: nfs-client
      accessModes: 
      - ReadWriteMany

      resources: 
        requests: 
          storage: 100Mi

  replicas: 2
  selector:
    matchLabels:
      app: redis-sts

  template:
    metadata:
      labels:
        app: redis-sts
    spec:
      containers:
      - image: redis:alpine
        name: redis
        ports:
        - containerPort: 6379

        volumeMounts:
        - name: redis-100m-pvc
          mountPath: /data
```

解释：
- StatefulSet 对象的名字是 redis-pv-sts，表示它使用了 PV 存储。
- 然后 `volumeClaimTemplates`里定义了一个 PVC，名字是 redis-100m-pvc，申请了 100MB 的 NFS 存储。
- 在 Pod 模板里用 `volumeMounts` 引用了这个 PVC，把网盘挂载到 Redis 的数据目录 /data 。

完整的 StatefulSet 对象，如下：

<img alt="full-statefulset" src="https://static001.geekbang.org/resource/image/1a/0f/1a06987c87f3db948b591883a81bac0f.jpg?wh=4000x2946" width="60%"/><br/>

检验：

```bash
k apply -f statefulset/redis-pv-sts.yml
k get sts
#
NAME           READY   AGE
redis-pv-sts   2/2     5s

k get pod
#
NAME             READY   STATUS    RESTARTS   AGE
redis-pv-sts-0   1/1     Running   0          11s
redis-pv-sts-1   1/1     Running   0          8s

k get pv
# 为每个 redis 的 Pod 分配了 PV
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                   STORAGECLASS   REASON   AGE
pvc-1115e2f4-93ac-464c-8070-7001b27abe21   100Mi      RWX            Delete           Bound    default/redis-100m-pvc-redis-pv-sts-0   nfs-client              90s
pvc-2c26d3d9-203c-4aad-b9c8-f164cb8d5b48   100Mi      RWX            Delete           Bound    default/redis-100m-pvc-redis-pv-sts-1   nfs-client              87s

# 添加 redis 键 message
kubectl exec -it redis-pv-sts-0 -- redis-cli
127.0.0.1:6379> set message 'hello world'
OK
127.0.0.1:6379> get message
"hello world"
127.0.0.1:6379> exit

# 模拟意外事故，删除 Redis 主实例
k delete pod redis-pv-sts-0
pod "redis-pv-sts-0" deleted

# 查看 pod
k get pod
NAME             READY   STATUS    RESTARTS   AGE
redis-pv-sts-0   1/1     Running   0          5s
redis-pv-sts-1   1/1     Running   0          6m51s

# 检查 message 键值
kubectl exec -it redis-pv-sts-0 -- redis-cli
127.0.0.1:6379> get message
"hello world"

# 设置 Redis 的 主/从 关系
kubectl exec -it redis-pv-sts-1 -- redis-cli replicaof redis-pv-sts-0.redis-svc 6379
kubectl exec -it redis-pv-sts-0 -- redis-cli info replication
# Replication
role:master
connected_slaves:1
slave0:ip=10.244.1.43,port=6379,state=online,offset=14,lag=0
master_failover_state:no-failover
#
kubectl exec -it redis-pv-sts-0 -- redis-cli set abc 1
OK
kubectl exec -it redis-pv-sts-1 -- redis-cli get abc
"1"
```

我们得到了预期的结果，完美！

### 小结/补充

- 使用 `clusterIP: None`，没有集群 IP 的 service 对象，也被称为 “headless service”。
- 即使有了 StatefulSet ，管理有状态的应用的难度依然很高，之后又有了 Operator 概念。

##
